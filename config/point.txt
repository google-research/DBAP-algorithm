{
    'job_name': 'test',
    'train_rl': True,
    'pretrain_rl': True,
    'pretrain_policy': True,
    'env_id': 'PointPlayEnv-v0',
    'num_epochs': 501,
    'num_eval_steps_per_epoch': 1000,
    'num_trains_per_train_loop': 1000,
    'num_expl_steps_per_train_loop': 1000,
    'min_num_steps_before_training': 1000,
    'max_path_length': 100,
    'batch_size': 1024,
    'replay_buffer_size': int(1e6),
    'layer_size': 256,
    'num_layers': 2,
    'algorithm': 'AWAC',
    'version': 'normal',
    'collection_mode': 'batch',
    'policy_class': 'GaussianPolicy',
    'policy_kwargs': {
        'hidden_sizes': (256, 256, 256, 256),
        'max_log_std': 0,
        'min_log_std': -6,
        'std_architecture': "values",
    },
    'qf_kwargs': {
        'hidden_sizes': (256, 256)
    },
    'trainer_kwargs': {
        'beta': [0.05, 0.1, 1.0],
        'discount': 0.99,
        'soft_target_tau': 5e-3,
        'target_update_period': 1,
        'policy_lr': 3E-4,
        'qf_lr': 3E-4,
        'reward_scale': 1,
        'alpha': 0,
        'use_automatic_entropy_tuning': False,
        'bc_num_pretrain_steps': 10000,
        'q_num_pretrain1_steps': 0,
        'q_num_pretrain2_steps': 10000,
        'policy_weight_decay': 1e-4,
        'train_bc_on_rl_buffer': False,
        'buffer_policy_sample_actions': False,

        'reparam_weight': 0.0,
        'awr_weight': 1.0,
        'bc_weight': 0.0,
        'compute_bc': False,
        'awr_use_mle_for_vf': False,
        'awr_sample_actions': False,
        'awr_min_q': True,
    },
    'path_loader_kwargs': {
        'demo_paths': (
            {
                'path': 'pointmass_play.pkl',
                'obs_dict': False,
                'is_demo': True,
                'train_split': .9,
            },
        ),
    },
    'path_loader_class': 'DictToMDPPathLoader',
    'use_validation_buffer': True,
    'add_env_demos': False,
    'add_env_offpolicy_data': False,
    'load_demos': True,
}
