{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('softlearning2': conda)",
   "display_name": "Python 3.7.7 64-bit ('softlearning2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "344694b620c1798f7363a6cc6c0c85978ab07f56ece39732dd6dfe3d19e58951"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from demo_2_awac import och_2_awac\n",
    "\n",
    "DATA_DIR = '/usr/local/google/home/bkinman/proj/rpl_reset_free/20201005_slider_play_reprocessed'\n",
    "\n",
    "def create_awac_dict_from_demo_pkls(data_dir):\n",
    "    full_awac_dict = None\n",
    "    glob_str = os.path.join(data_dir, 'recording?.pkl')\n",
    "    pkl_files = [f for f in glob.glob(glob_str)]\n",
    "    for f in pkl_files:\n",
    "        data_path = os.path.join(data_dir, f)\n",
    "        data = pickle.load(open(data_path,'rb'))\n",
    "        awac_formatted_list = och_2_awac(data)\n",
    "        for entry_dict in awac_formatted_list:\n",
    "            if not full_awac_dict:\n",
    "                full_awac_dict = entry_dict \n",
    "            else:\n",
    "                for k, v in entry_dict.items():\n",
    "                    full_awac_dict[k] = np.concatenate((full_awac_dict[k], v), axis=0)\n",
    "    return full_awac_dict\n",
    "\n",
    "def relabel_bc(trajs: dict, window_size = 100):\n",
    "    goal_size = 25\n",
    "    paths = []\n",
    "    num_idxs = trajs['observations'].shape[0]\n",
    "    for idx_start in range(num_idxs - window_size - 1):\n",
    "        path = {}\n",
    "        # Windowed observations\n",
    "        ob = trajs['observations'][idx_start:idx_start + window_size].copy()\n",
    "        next_ob = trajs['observations'][idx_start + 1:idx_start + window_size + 1].copy()\n",
    "        # Last observations\n",
    "        goals = np.repeat([trajs['observations'][idx_start + window_size - 1]], len(ob), axis=0)\n",
    "        ob[:, goal_size:] = goals[:, :goal_size]\n",
    "        next_ob[:, goal_size:] = goals[:, :goal_size]\n",
    "        path['observations'] = ob.copy()\n",
    "        path['full_observations'] = ob.copy()\n",
    "        path['next_observations'] = next_ob.copy()\n",
    "        path['full_next_observations'] = next_ob.copy()\n",
    "        path['actions'] = trajs['actions'][idx_start:idx_start + window_size].copy()\n",
    "        reward = np.zeros((len(ob), 1))\n",
    "        reward[-1] = 1.0\n",
    "        path['rewards'] = reward\n",
    "        terminals = np.zeros((len(ob),), dtype=np.bool)\n",
    "        terminals[-1] = True\n",
    "        path['terminals'] = terminals\n",
    "        path['env_infos'] = [{}]*len(ob)\n",
    "        path['agent_infos'] = [{}] * len(ob)\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def relabel_bc_strided(trajs: dict, window_size = 100, stride=10):\n",
    "    \"\"\" Strided relabeling procedure produces less data.\"\"\"\n",
    "    goal_size = 25\n",
    "    paths = []\n",
    "    num_idxs = trajs['observations'].shape[0]\n",
    "    for idx_start in range(num_idxs - window_size - 1):\n",
    "        path = {}\n",
    "        # Windowed observations\n",
    "        ob = trajs['observations'][idx_start:idx_start + window_size][::stride].copy()\n",
    "        next_ob = trajs['observations'][idx_start + 1:idx_start + window_size + 1][::stride].copy()\n",
    "        # Last observations\n",
    "        goals = np.repeat([trajs['observations'][idx_start + window_size - 1]], len(ob), axis=0)\n",
    "        ob[:, goal_size:] = goals[:, :goal_size]\n",
    "        next_ob[:, goal_size:] = goals[:, :goal_size]\n",
    "        path['observations'] = ob.copy()\n",
    "        path['full_observations'] = ob.copy()\n",
    "        path['next_observations'] = next_ob.copy()\n",
    "        path['full_next_observations'] = next_ob.copy()\n",
    "        path['actions'] = trajs['actions'][idx_start:idx_start + window_size][::stride].copy()\n",
    "        reward = np.zeros((len(ob), 1))\n",
    "        reward[-1] = 1.0\n",
    "        path['rewards'] = reward\n",
    "        terminals = np.zeros((len(ob),), dtype=np.bool)\n",
    "        terminals[-1] = True\n",
    "        path['terminals'] = terminals\n",
    "        path['env_infos'] = [{}]*len(ob)\n",
    "        path['agent_infos'] = [{}] * len(ob)\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def compute_window_size(obs, thresh_low=4.45, thresh_high=10e6, debug_plot = False):\n",
    "    \"\"\" Computes the window size, which is the essentially the average duration of each episode.\n",
    "    \"\"\"\n",
    "    thresh = ((obs > thresh_low) & (obs < thresh_high))*1.0\n",
    "    grad = np.gradient(thresh)\n",
    "    last_rise = 0\n",
    "    deltas_ts = []\n",
    "    for ts in range(len(grad)):\n",
    "        if grad[ts] > 0:\n",
    "            last_rise = ts\n",
    "        if grad[ts] < 0:\n",
    "            deltas_ts.append((last_rise, ts))\n",
    "    mid_ts = np.array([((b-a)/2+a) for a,b in deltas_ts]).astype(np.int32)\n",
    "    if debug_plot:\n",
    "        mid_pnts = np.zeros(len(obs))\n",
    "        mid_pnts[mid_ts] = 1\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        plt.plot(thresh)\n",
    "        plt.plot(mid_pnts)\n",
    "        plt.plot(obs-np.amin(obs))\n",
    "    mean_episode_len = np.mean(mid_ts[1:] - mid_ts[:-1])\n",
    "    return int(mean_episode_len)"
   ]
  },
  {
   "source": [
    "## Load demo data, convert to AWAC format, and relabel for Behavior Cloning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bc_num_pretrain_steps should be 789000 steps\nq_num_pretrain2_steps should be 1578000 steps\n"
     ]
    }
   ],
   "source": [
    "full_awac_dict = create_awac_dict_from_demo_pkls(DATA_DIR)\n",
    "sliding_cabinet_obs = full_awac_dict['observations'][:,1]\n",
    "window_size = compute_window_size(sliding_cabinet_obs)\n",
    "bc_training_data = relabel_bc_strided(full_awac_dict, window_size)\n",
    "lens = sum(len(a['observations']) for a in bc_training_data)\n",
    "print(f'bc_num_pretrain_steps should be {(int(lens/1000)+1)*1500} steps')\n",
    "print(f'q_num_pretrain2_steps should be {(int(lens/1000)+1)*3000} steps')\n",
    "output_path = os.path.join(DATA_DIR, 'bc_train_strided.pkl')\n",
    "pickle.dump(bc_training_data, open(output_path,'wb'))"
   ]
  },
  {
   "source": [
    "## Reprocess Demo Data\n",
    "When the demo data was collected, the observation vector was the incorrect size, and contained incorrect values (should have been zero initialized).\n",
    "The following routine opens the original demo vectors and corrects this. To prevent accidental overwriting of data, data will be dumped to a new directory alongside DATA_DIR."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess_demo_data(data_dir):\n",
    "    output_dirname = os.path.basename(os.path.normpath(DATA_DIR))+'_reprocessed'\n",
    "    output_dir = os.path.join(DATA_DIR, '..', output_dirname)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    glob_str = os.path.join(DATA_DIR, 'recording?.pkl')\n",
    "    pkl_files = [f for f in glob.glob(glob_str)]\n",
    "    for f in pkl_files:\n",
    "        data = pickle.load(open(f, 'rb'))\n",
    "        for episode in data:\n",
    "            for step in episode:\n",
    "                step['obs'] = np.concatenate((step['obs'][:25], np.zeros(25)), axis=0)\n",
    "        pickle.dump(data, open(os.path.join(output_dir, os.path.basename(f)), 'wb'))\n",
    "reprocess_demo_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}